{
	"name": "Spark Transformations sn",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "JDSparkPool289",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e3d06722-6104-4b3f-9725-ed0bb833f14f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/e2d3f9d2-fe25-4a6a-bccd-16e5d2933d82/resourceGroups/rgday3/providers/Microsoft.Synapse/workspaces/secwkspce289/bigDataPools/JDSparkPool289",
				"name": "JDSparkPool289",
				"type": "Spark",
				"endpoint": "https://secwkspce289.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/JDSparkPool289",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# **concat**\r\n",
					"Concatenates multiple input columns together into a single column"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql.functions import *\r\n",
					"from pyspark.sql import functions as F\r\n",
					"from pyspark.sql.window import Window"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"data = [('James','','Smith','1991-04-01','M',3000),\r\n",
					"  ('Michael','Rose','','2000-05-19','M',4000),\r\n",
					"  ('Robert','','Williams','1978-09-05','M',4000),\r\n",
					"  ('Maria','Anne','Jones','1967-12-01','F',4000),\r\n",
					"  ('Jen','Mary','Brown','1980-02-17','F',-1)\r\n",
					"]\r\n",
					" \r\n",
					"columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\r\n",
					"df = spark.createDataFrame(data=data, schema = columns)\r\n",
					"display(df)"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"#from pyspark.sql.functions import concat, col, lit\r\n",
					"df = df.withColumn(\"concat_new\",F.concat(col(\"firstname\"), lit(\" \"), col(\"lastname\")))\r\n",
					"#df = df.withColumn(\"concat_new\",concat(col(\"firstname\"), lit(\" \"), col(\"lastname\")))\r\n",
					"display(df)"
				],
				"execution_count": 26
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# **split**\r\n",
					"split(): The split() is used to split a string column of the dataframe into multiple columns. This function is applied to the dataframe with the help of withColumn() and select()."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import split\r\n",
					"df = df.withColumn('Name1', split(df['concat_new'], \" \").getItem(0)).withColumn('Name2', split(df['concat_new'], \" \").getItem(1))\r\n",
					"display(df)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**when():** The when the function is used to display the output based on the particular condition. It evaluates the condition provided and then returns the values accordingly. It is a SQL function that supports PySpark to check multiple conditions in a sequence and return the value. This function similarly works as if-then-else and switch statements."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import when\r\n",
					"display(df.withColumn(\"salary > 3000\", when(df.salary > 3000, \"Yes\").otherwise('No')))"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Drop duplicates"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"data = [('James','','Smith','1991-04-01','M',3000),\r\n",
					"  ('Michael','Rose','','2000-05-19','M',4000),\r\n",
					"  ('Robert','','Williams','1978-09-05','M',4000),\r\n",
					"  ('Maria','Anne','Jones','1967-12-01','F',4000),\r\n",
					"  ('Jen','Mary','Brown','1980-02-17','F',-1),\r\n",
					"  ('Jen','Mary','Brown','1980-02-17','F',-1)\r\n",
					"]\r\n",
					" \r\n",
					"columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\r\n",
					"df = spark.createDataFrame(data=data, schema = columns)\r\n",
					"display(df)"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(df.drop_duplicates())"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Union\r\n",
					"union() method of the DataFrame is used to merge two DataFrameâ€™s of the same structure/schema. If schemas are not the same it returns an error."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\r\n",
					"    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\r\n",
					"    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\r\n",
					"    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\r\n",
					"  ]\r\n",
					" \r\n",
					"columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\r\n",
					"df = spark.createDataFrame(data = simpleData, schema = columns)\r\n",
					"display(df)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\r\n",
					"    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\r\n",
					"    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\r\n",
					"    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\r\n",
					"    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\r\n",
					"  ]\r\n",
					"columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\r\n",
					" \r\n",
					"df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\r\n",
					"display(df2)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"unionDF = df.union(df2)\r\n",
					"display(unionDF)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Pivot operation\r\n",
					"PySpark SQL provides pivot() function to rotate the data from one column into multiple columns. It is an aggregation where one of the grouping columns values is transposed into individual columns with distinct data."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\r\n",
					"      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\r\n",
					"      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\r\n",
					"      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\r\n",
					" \r\n",
					"columns= [\"Product\",\"Amount\",\"Country\"]\r\n",
					"df = spark.createDataFrame(data = data, schema = columns)\r\n",
					"display(df)\r\n",
					""
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\r\n",
					"display(pivotDF)"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# expr\r\n",
					"expr() function takes SQL expression as a string argument, executes the expression, and returns a PySpark Column type If you have SQL background, you pretty much familiar using || to concatenate values from two string columns, you can use expr() expression to do exactly same."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"data=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")] \r\n",
					"df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \r\n",
					"display(df)"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(df.withColumn(\"Name\",expr(\" col1 ||','|| col2\")))"
				],
				"execution_count": 14
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# substring()\r\n",
					"In PySpark, the substring() function is used to extract the substring from a DataFrame string column by providing the position and length of the string you wanted to extract."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"data = [(1,\"20200828\"),(2,\"20180525\")]\r\n",
					"columns=[\"id\",\"date\"]\r\n",
					"df=spark.createDataFrame(data,columns)\r\n",
					"df = df.withColumn('year', substring('date', 1,4))\\\r\n",
					"    .withColumn('month', substring('date', 5,2))\\\r\n",
					"    .withColumn('day', substring('date', 7,2))\r\n",
					"display(df)"
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# to_date()\r\n",
					"function to convert TimestampType (or string) column to DateType column. The input to this function should be timestamp column or string in TimestampType format and it returns just date in DateType column"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df=spark.createDataFrame(\r\n",
					"        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\r\n",
					"        schema=[\"id\",\"input_timestamp\"])\r\n",
					"display(df)"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df.printSchema()"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					" \r\n",
					"#Timestamp String to DateType\r\n",
					"df = df.withColumn(\"date_type\",to_date(\"input_timestamp\"))\r\n",
					"display(df)"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df.printSchema()"
				],
				"execution_count": 19
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# date_format\r\n",
					"use date_format() function to convert the DataFrame column from Date to String format"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					" \r\n",
					"df=spark.createDataFrame([[\"1\"]],[\"id\"])\r\n",
					"df = df.select(current_date().alias(\"current_date\"), \\\r\n",
					"      date_format(current_timestamp(),\"yyyy MM dd\").alias(\"yyyy MM dd\"), \\\r\n",
					"      date_format(current_timestamp(),\"MM/dd/yyyy hh:mm\").alias(\"MM/dd/yyyy\"), \\\r\n",
					"      date_format(current_timestamp(),\"yyyy MMM dd\").alias(\"yyyy MMMM dd\"), \\\r\n",
					"      date_format(current_timestamp(),\"yyyy MMMM dd E\").alias(\"yyyy MMMM dd E\") \\\r\n",
					"   )\r\n",
					"display(df)"
				],
				"execution_count": 20
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# datediff() : the difference between two dates"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					"data = [(\"1\",\"2019-07-01\"),(\"2\",\"2019-06-24\"),(\"3\",\"2019-08-24\")]\r\n",
					"df=spark.createDataFrame(data=data,schema=[\"id\",\"date\"])\r\n",
					" \r\n",
					"display(df.select(\r\n",
					"      col(\"date\"),\r\n",
					"      current_date().alias(\"current_date\"),\r\n",
					"      datediff(current_date(),col(\"date\")).alias(\"datediff\")\r\n",
					"    ))"
				],
				"execution_count": 21
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# months_between() : calculate month and year differences between two dates"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql.functions import *\r\n",
					" \r\n",
					"from pyspark.sql.functions import *\r\n",
					"data = [(\"1\",\"2019-07-01\"),(\"2\",\"2019-06-24\"),(\"3\",\"2019-08-24\")]\r\n",
					"df=spark.createDataFrame(data=data,schema=[\"id\",\"date\"])\r\n",
					" \r\n",
					"display(df.withColumn(\"datesDiff\", datediff(current_date(),col(\"date\"))) \\\r\n",
					"  .withColumn(\"montsDiff\", months_between(current_date(),col(\"date\"))) \\\r\n",
					"  .withColumn(\"montsDiff_round\",round(months_between(current_date(),col(\"date\")),2)) \\\r\n",
					"  .withColumn(\"yearsDiff\",months_between(current_date(),col(\"date\"))/lit(12)) \\\r\n",
					"  .withColumn(\"yearsDiff_round\",round(months_between(current_date(),col(\"date\"))/lit(12),2)))"
				],
				"execution_count": 22
			}
		]
	}
}